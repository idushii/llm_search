# Подробное описание проекта "Mind Search"

## Общее назначение

"Mind Search" представляет собой интеллектуальную систему поиска и обработки информации, разработанную для глубокого и всестороннего анализа заданных тем. Система выполняет многоуровневый поиск по запросу пользователя, обрабатывает найденную информацию и генерирует структурированный, содержательный ответ на основе наиболее релевантных источников.

## Архитектура системы

Система построена по модульному принципу и включает следующие основные компоненты:

### 1. Модули обработки данных

- **Core (Ядро)** - базовые компоненты и утилиты:
  - `config.py` - конфигурация и переменные окружения
  - `constants.py` - константы проекта
  - `utils.py` - вспомогательные функции (хеширование, форматирование, логирование)

- **Search (Поиск)** - компоненты, ответственные за поиск информации:
  - `planned_topics.py` - планирование подзапросов на основе исходного запроса
  - `planned_searching.py` - генерация поисковых запросов для каждого подзапроса
  - `scraping.py` - выполнение поисковых запросов и скрапинг веб-страниц

- **Processing (Обработка)** - компоненты для обработки найденной информации:
  - `ranking_search_result.py` - ранжирование результатов поиска
  - `summarizer.py` - создание сжатых саммари документов
  - `ranking_summary.py` - ранжирование саммари по релевантности
  - `nlp_utils.py` - генерация итогового структурированного ответа

- **Storage (Хранение)** - управление данными и кэширование:
  - `cache_manager.py` - управление кэшированием данных
  - `file_system.py` - работа с файловой системой проекта

### 2. Структура каталогов

```
/project/
    /src/
        /core/        - базовые компоненты
        /search/      - модули поиска
        /processing/  - модули обработки
        /storage/     - модули управления данными
    /cache/           - кэш данных
        /docs/        - кэшированные документы
        /search_queries/  - кэшированные поисковые запросы
        /search_results/  - кэшированные результаты поиска
        /ranked_results/  - кэшированные ранжированные результаты
        /ranked_summaries/ - кэшированные ранжированные саммари
    main.py           - точка входа в приложение
    requirements.txt  - зависимости
    .env.example      - шаблон переменных окружения
    .gitignore        - исключения для Git
    README.md         - документация
```

## Принцип работы

Система работает в несколько последовательных этапов:

### 1. Планирование запросов

- Пользователь вводит исходный запрос через консольный интерфейс
- `TopicPlanner` анализирует запрос с помощью LLM и генерирует подзапросы (до 3-х), охватывающие различные аспекты темы, используя специальный промпт:
  ```
Ты – ассистент, который составляет до 3 подзапросов для поиска в интернете по заданной теме. Твои подзапросы должны соответствовать следующим критериям:

Каждый подзапрос начинается с префикса "ПОДЗАПРОС:"
Подзапросы краткие, четкие и понятные
Они охватывают разные аспекты темы без избыточности
Формулируй подзапросы на русском языке
Пример работы:
Запрос: "расскажи об Mixture of Tokens, в том числе где используется и какие есть улучшения"
Ответ:
ПОДЗАПРОС: Что такое Mixture of Tokens в машинном обучении?
ПОДЗАПРОС: Где используется Mixture of Tokens и какие задачи решает?
ПОДЗАПРОС: Какие есть улучшения Mixture of Tokens и их преимущества?

Составь подзапросы в таком формате.
  ```
- Пользователь может просмотреть и отредактировать сгенерированные подзапросы
- `SearchQueryPlanner` создает поисковые запросы для каждого подзапроса на русском и английском языках, используя другой специализированный промпт:
  ```
Ты – интеллектуальный помощник, который составляет поисковые запросы на русском и английском языках. 
Твоя задача – создать до 3 запросов на обеих языках по заданной теме. 
Формат каждого запроса:

- Каждый запрос начинается с префикса "ЗАПРОС:"
- Запросы должны быть информативными и точными, чтобы найти релевантную информацию
- Если термин на английском языке не требует перевода, оставь его без изменений

Пример вывода для темы "Mixture of Tokens в машинном обучении":

ЗАПРОС: Что такое Mixture of Tokens в машинном обучении?
ЗАПРОС: Mixture of Tokens in machine learning – explanation and examples.
  ```

### 2. Поиск информации

- `SearchEngine` выполняет поисковые запросы через API поисковой системы SearchXNG с применением Rate Limiting для соблюдения ограничений API
- Для каждого запроса собираются метаданные результатов (заголовок, URL, сниппет)

### 3. Ранжирование результатов поиска

- После получения списка результатов от поисковой системы, `SearchResultRanker` оценивает релевантность каждого результата исходному запросу с использованием LLM
- Ранжирование выполняется на основе пяти критериев, каждый оценивается по шкале от 0 до 10:
  1. **Соответствие исходному запросу (0-10)** – насколько результат отвечает на поставленный вопрос
  2. **Соответствие направлению поиска (0-10)** – насколько результат соответствует текущему подзапросу
  3. **Полнота информации (0-10)** – охватывает ли результат ключевые аспекты темы
  4. **Точность (0-10)** – содержит ли результат корректные, актуальные и точные данные
  5. **Читабельность и структура (0-10)** – насколько текст логично организован и легко воспринимается
- Итоговая оценка каждого результата рассчитывается как среднее значение по пяти критериям
- Выбираются топ-5 наиболее релевантных результатов для дальнейшей обработки

#### Массовая оценка результатов поиска с пагинацией
- В отличие от предыдущей версии, которая оценивала результаты поочередно, новая система выполняет массовую оценку с пагинацией:
  - Результаты группируются пакетами по 10 штук для одновременной обработки
  - Для каждой группы из 10 результатов формируется единый запрос к LLM для оценки
  - Это значительно ускоряет процесс ранжирования и снижает количество API-запросов
  - Система автоматически обрабатывает все страницы результатов, пока не оценит все найденные ссылки

- При массовой оценке используется специальный промпт:
  ```
#### **Задача:**  
Оценить релевантность каждого из предоставленных результатов поиска относительно исходного запроса и текущего направления поиска.

#### **Критерии оценки для каждого результата:**  
1. **Соответствие исходному запросу (0-10)** – насколько результат отвечает на поставленный вопрос.  
2. **Соответствие направлению поиска (0-10)** – насколько результат соответствует текущему направлению исследования.  
3. **Полнота информации (0-10)** – охватывает ли результат ключевые аспекты темы.  
4. **Точность (0-10)** – содержит ли результат корректные, актуальные и точные данные.  
5. **Читабельность и структура (0-10)** – насколько контент логично организован и легко воспринимается.  

#### **Формат вывода:**  
Для каждого результата верни оценки в формате JSON в массиве. Заголовок результата должен соответствовать его заголовку в списке. Итоговый рейтинг рассчитывается как среднее значение по пяти критериям (округленное до одного знака после запятой).

```json
[
  {
    "заголовок": "заголовок первого результата поиска",
    "соответствие_запросу": N,
    "соответствие_направлению": N,
    "полнота": N,
    "точность": N,
    "структура": N,
    "итоговый_рейтинг": N
  },
  {
    "заголовок": "заголовок второго результата поиска",
    "соответствие_запросу": N,
    "соответствие_направлению": N,
    "полнота": N,
    "точность": N,
    "структура": N,
    "итоговый_рейтинг": N
  }
  ... остальные результаты ...
]
```

Где N - оценка от 0 до 10. Используй только числа, без объяснений.
  ```

- Особенности работы с заголовками вместо ID:
  - Каждый результат идентифицируется по заголовку, что делает оценки более понятными
  - Система автоматически сопоставляет оценки с результатами поиска на основе заголовков
  - В случае если LLM пропустил какой-либо результат, система присваивает ему оценку по умолчанию
  - Такой подход позволяет более надежно связывать результаты и их оценки, даже если порядок в ответе LLM отличается

- Основные преимущества массовой оценки с пагинацией:
  - Значительное сокращение времени обработки больших наборов результатов (до 5-10 раз быстрее)
  - Экономия на количестве API-запросов (в 10 раз меньше запросов)
  - Более согласованная оценка результатов в рамках одной группы
  - Возможность сравнения результатов непосредственно друг с другом в одном контексте

- Результаты ранжирования сохраняются в кэш для повторного использования

### 4. Скрапинг и сохранение

- После определения топ-5 наиболее релевантных результатов, производится скрапинг содержимого этих веб-страниц с использованием сервиса r.jina.ai
- r.jina.ai возвращает уже готовый очищенный текст в формате Markdown
- Скачанные документы сохраняются в кэш в директории `/cache/docs/{{theme_name}}/{{doc_name}}`
- В случае ошибок при скрапинге через r.jina.ai имеется резервный механизм прямого скрапинга и преобразования HTML в Markdown

### 5. Саммаризация документов

- `DocumentSummarizer` передает скачанный Markdown-текст языковой модели для саммаризации
- Для каждого документа генерируется сжатое саммари, выделяющее ключевую информацию, согласно промпту:
  ```
Роль: Ты — интеллектуальный ассистент, специализирующийся на анализе и кратком изложении текстов. Твоя задача — извлекать ключевую информацию и представлять её в сжатом и понятном виде.

Требования к summary:

Сжатость: Используй только важные факты, избегая лишних деталей.
Ясность: Формулируй мысли четко и логично, избегая двусмысленности.
Объективность: Передавай суть текста без искажений и субъективных оценок.
Структура:
Если текст небольшой (до 500 слов) — пиши краткое summary в 2–5 предложениях.
Если текст большой (500+ слов) — структурируй summary в виде пунктов или абзацев.
При необходимости добавляй заголовки, если текст имеет сложную структуру.
Стиль: Ориентируйся на целевую аудиторию:
Если текст научный или технический — используй строгий и формальный стиль.
Если текст новостной — передавай основные факты кратко и объективно.
Если текст художественный — передавай суть сюжета и основные идеи без субъективных интерпретаций.

Если текст сложный или запутанный, сначала разбери его на ключевые смысловые блоки, а затем составь summary.
  ```
- Саммари сохраняются в кэш для повторного использования в директории `/cache/summaries/{{theme_name}}/{{doc_name}}`

### 6. Ранжирование саммари

- `SummaryRanker` оценивает релевантность каждого саммари исходному запросу с использованием LLM
- Ранжирование выполняется по пяти критериям, аналогичным критериям ранжирования результатов поиска:
  1. **Соответствие исходному запросу (0-10)** 
  2. **Соответствие направлению поиска (0-10)** 
  3. **Полнота информации (0-10)** 
  4. **Точность (0-10)** 
  5. **Читабельность и структура (0-10)** 
- В случае ошибок при обращении к LLM используется резервный алгоритм ранжирования на основе ключевых слов
- Итоговая оценка каждого саммари рассчитывается как среднее значение по всем критериям
- Выбираются топ-5 наиболее информативных саммари
- Результаты ранжирования сохраняются в кэш для повторного использования

### 7. Генерация ответа

- `AnswerGenerator` объединяет информацию из полных текстов документов, саммари которых попали в топ-5 по результатам ранжирования
- Формирует промпт для LLM, включающий:
  - Исходный запрос пользователя
  - Полные тексты документов (не саммари), которые получили высокий рейтинг
  - Требования к структуре и формату ответа
- Создается структурированный ответ в формате Markdown с разделами и подразделами
- Формируется список использованных источников с URL
- Ответ и исходный запрос сохраняются в кэш

### 8. Кэширование и управление данными

- `CacheManager` сохраняет все промежуточные результаты в кэш
- Система использует следующую структуру кэширования:
  - `/cache/search_queries/` - кэшированные поисковые запросы
  - `/cache/search_results/` - кэшированные результаты поиска
  - `/cache/docs/` - кэшированные документы
  - `/cache/ranked_results/` - кэшированные ранжированные результаты
  - `/cache/ranked_summaries/` - кэшированные ранжированные саммари
- `FileSystemManager` обеспечивает работу с файловой системой (экспорт/импорт, удаление)
- Система автоматически очищает устаревшие данные кэша после указанного периода времени (по умолчанию 30 дней)
- Предусмотрена возможность получения подробной статистики по кэшу

## Технические особенности

### API и интеграции

- **LLM API** (AITUNNEL_API_KEY) - используется для:
  - Генерации подзапросов
  - Генерации поисковых запросов
  - Ранжирования результатов поиска
  - Саммаризации документов
  - Ранжирования саммари
  - Создания итогового ответа

- **SearchXNG API** - поисковая система для выполнения запросов

- **Jina AI API (r.jina.ai)** - используется для скрапинга веб-страниц и преобразования их в чистый Markdown-текст, что исключает необходимость дополнительной обработки HTML

### Ограничение нагрузки (Rate Limiting)

- Встроенный механизм ограничения частоты запросов к API:
  - SearchXNG: до 10 запросов в минуту (настраивается через LIMIT_SEARCHXNG_RPM)
  - Jina AI: до 5 запросов в секунду (настраивается через LIMIT_JINA_RPS)
  - AITUNNEL: до 2 запросов в секунду (настраивается через LIMIT_AITUNNEL_RPS)

### Асинхронная обработка

- Использование `asyncio` и `aiohttp` для параллельного выполнения запросов
- Применение семафоров для контроля одновременных запросов к каждому сервису
- Повышенная эффективность при работе с множественными запросами

### Интеллектуальное ранжирование через LLM

- Ранжирование результатов поиска с помощью языковой модели по пяти критериям с оценкой от 0 до 10
- Массовая оценка результатов группами по 10 с использованием пагинации, что значительно ускоряет процесс
- Ранжирование саммари также выполняется с использованием языковой модели по пяти критериям
- Резервные алгоритмы ранжирования на основе ключевых слов на случай ошибок при обращении к LLM
- Итоговая оценка рассчитывается как среднее значение по всем критериям

### Кэширование и управление данными

- Структурированное кэширование всех этапов обработки через специальные директории
- Уникальные идентификаторы тем на основе запроса и хеша
- Автоматическая очистка устаревших данных
- Детальная статистика использования кэша
- Система .gitignore для контроля версий, сохраняющая структуру директорий, но исключающая содержимое кэша

## Практическое применение

Система особенно эффективна для:

1. **Исследования сложных тем**, требующих анализа множества источников
2. **Образовательных целей**, когда необходимо получить структурированную информацию по определенной теме
3. **Аналитической работы**, когда требуется обработать большие объемы информации из интернета
4. **Подготовки материалов** для статей, докладов, презентаций

## Технические требования

- **Python 3.8+**
- **API ключи**:
  - AITUNNEL_API_KEY для доступа к языковой модели
  - AITUNNEL_MODEL для указания используемой модели
  - SEARCHXNG_BASIC_AUTH_LOGIN и SEARCHXNG_BASIC_AUTH_PASSWORD для доступа к поисковой системе
- **Зависимости**:
  - requests==2.31.0
  - python-dotenv==1.0.0
  - aiohttp==3.8.5
  - asyncio==3.4.3

## Ограничения и особенности использования

- Требуется стабильное интернет-соединение
- Производительность зависит от доступности и быстродействия внешних API
- Количество запросов ограничено лимитами API-сервисов
- Качество итогового ответа зависит от качества найденных источников
- Для обработки результатов поиска и ранжирования используется LLM, что обеспечивает высокое качество оценки, но увеличивает стоимость запросов
- Конфигурируемые параметры позволяют настраивать работу системы под конкретные требования

## Пример процесса работы

### Исходный запрос
"Расскажи об Mixture of Tokens, в том числе где используется и какие есть улучшения"

### Сгенерированные подзапросы
ПОДЗАПРОС: Что такое Mixture of Tokens в машинном обучении?
ПОДЗАПРОС: Где используется Mixture of Tokens и какие задачи решает?
ПОДЗАПРОС: Какие есть улучшения Mixture of Tokens и их преимущества?

### Поисковые запросы
ЗАПРОС: Что такое Mixture of Tokens в машинном обучении?
ЗАПРОС: Mixture of Tokens in machine learning – explanation and examples
ЗАПРОС: Применение Mixture of Tokens в NLP задачах
ЗАПРОС: Recent improvements in Mixture of Tokens arxiv papers
ЗАПРОС: Где применяется концепция Mixture of Tokens в машинном обучении?
ЗАПРОС: Research paper on Mixture of Experts (MoE) models in machine learning applications

### Ранжирование и скрапинг
После получения результатов поиска система:
1. Ранжирует все найденные результаты с помощью LLM
2. Выбирает топ-5 наиболее релевантных результатов
3. Только для этих топ-5 результатов производится скрапинг через r.jina.ai
4. Скачанные документы сохраняются в кэш

### Итоговый ответ
Система формирует структурированный ответ в формате Markdown на основе отранжированных через LLM саммари, включающий введение, основные разделы по теме и список использованных источников.
