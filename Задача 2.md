Необходимо добавить сохранение embedding в базу данных vectordb и использовать ее для поиска похожих документов в момент генерации итогового ответа.
При этом надо использовать модель для генерации embedding, которая будет использоваться для поиска похожих документов.

### Промпты

#### Для планирования подзапросов

```
Ты – ассистент, который составляет до 3 подзапросов для поиска в интернете по заданной теме. Твои подзапросы должны соответствовать следующим критериям:

Каждый подзапрос начинается с префикса "ПОДЗАПРОС:"
Подзапросы краткие, четкие и понятные
Они охватывают разные аспекты темы без избыточности
Формулируй подзапросы на русском языке
Пример работы:
Запрос: "расскажи об Mixture of Tokens, в том числе где используется и какие есть улучшения"
Ответ:
ПОДЗАПРОС: Что такое Mixture of Tokens в машинном обучении?
ПОДЗАПРОС: Где используется Mixture of Tokens и какие задачи решает?
ПОДЗАПРОС: Какие есть улучшения Mixture of Tokens и их преимущества?

Составь подзапросы в таком формате.
```

#### Для составления запросов для поиска в интернете

```
Ты – интеллектуальный помощник, который составляет поисковые запросы на русском и английском языках. 
Твоя задача – создать до 3 запросов на обеих языках по заданной теме. 
Формат каждого запроса:

- Каждый запрос начинается с префикса "ЗАПРОС:"
- Запросы должны быть информативными и точными, чтобы найти релевантную информацию
- Один из запросов может включать ключевые слова для поиска академических или технических источников (например, "research paper", "arxiv", "IEEE")
- Если термин на английском языке не требует перевода, оставь его без изменений

Пример вывода для темы "Mixture of Tokens в машинном обучении":

ЗАПРОС: Что такое Mixture of Tokens в машинном обучении?
ЗАПРОС: Mixture of Tokens in machine learning – explanation and examples

```

#### Для саммаризации

```
Роль: Ты — интеллектуальный ассистент, специализирующийся на анализе и кратком изложении текстов. Твоя задача — извлекать ключевую информацию и представлять её в сжатом и понятном виде.

Требования к summary:

Сжатость: Используй только важные факты, избегая лишних деталей.
Ясность: Формулируй мысли четко и логично, избегая двусмысленности.
Объективность: Передавай суть текста без искажений и субъективных оценок.
Структура:
Если текст небольшой (до 500 слов) — пиши краткое summary в 2–5 предложениях.
Если текст большой (500+ слов) — структурируй summary в виде пунктов или абзацев.
При необходимости добавляй заголовки, если текст имеет сложную структуру.
Стиль: Ориентируйся на целевую аудиторию:
Если текст научный или технический — используй строгий и формальный стиль.
Если текст новостной — передавай основные факты кратко и объективно.
Если текст художественный — передавай суть сюжета и основные идеи без субъективных интерпретаций.

Если текст сложный или запутанный, сначала разбери его на ключевые смысловые блоки, а затем составь summary.
```

### Как это работает

1. В момент поиска генерируется подзапросы для раскрытия темы целевого запроса.
2. Для каждого подзапроса генерируется запросы для поиска в интернете - запросы, которые должны найти то же, но другими словами и на русском и на английском.
3. Выполняется поиск в интернете по всем полученным запросам для интеренета.
4. Результаты поиска фильтруется на дубликаты.
5. Составляется рейтинг для найденных кратких описаний из интеренета на основании релевантности подзапросу и целевому запросу.
6. Выбирается топ 5.
7. Каждый из найденных сайтов скачивается и сохраняется в /cache/docs/{{theme_name}}/{{doc_name}}.
8. Для каждого документа выполняется саммаризация и сохраняется в /cache/summaries/{{theme_name}}/{{doc_name}}.
9. Для всех полученных саммари за все подзапросы выставляется рейтинг на основании релевантности целевому запросу.
10. Выбирается топ 5 саммари.
11. На основании найденных документов из топ 5 саммари генерируется ответ целевому запросу.


### Embedding
``` python
# Установка: pip install sentence-transformers
from sentence_transformers import SentenceTransformer

model = SentenceTransformer('all-MiniLM-L6-v2')  # Лёгкая модель (384D)
texts = ["Пример текста", "Другой текст для обработки"]

embeddings = model.encode(texts, normalize_embeddings=True)  # (2, 384)
```


### Vectordb
``` python
from vectordb import InMemoryExactNNVectorDB
import numpy as np

# Определение структуры данных
from docarray import BaseDoc
from docarray.typing import NdArray

class MyDoc(BaseDoc):
    embedding: NdArray[128]

# Инициализация базы
db = InMemoryExactNNVectorDB[MyDoc](workspace='./db')

# Добавление данных
docs = [MyDoc(embedding=np.random.rand(128)) for _ in range(1000)]
db.index(docs)

# Поиск
query = MyDoc(embedding=np.random.rand(128))
results = db.search([query], limit=5)
```

### Саммаризация

``` python
!pip install transformers sentencepiece datasets

from transformers import AutoTokenizer, T5ForConditionalGeneration

model_name = "IlyaGusev/rut5_base_sum_gazeta"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = T5ForConditionalGeneration.from_pretrained(model_name)

article_text = "Высота башни составляет 324 метра (1063 фута), примерно такая же высота, как у 81-этажного здания, и самое высокое сооружение в Париже. Его основание квадратно, размером 125 метров (410 футов) с любой стороны. Во время строительства Эйфелева башня превзошла монумент Вашингтона, став самым высоким искусственным сооружением в мире, и этот титул она удерживала в течение 41 года до завершения строительство здания Крайслер в Нью-Йорке в 1930 году. Это первое сооружение которое достигло высоты 300 метров. Из-за добавления вещательной антенны на вершине башни в 1957 году она сейчас выше здания Крайслер на 5,2 метра (17 футов). За исключением передатчиков, Эйфелева башня является второй самой высокой отдельно стоящей структурой во Франции после виадука Мийо."


input_ids = tokenizer(
    [article_text],
    add_special_tokens=True,
    padding="max_length",
    truncation=True,
    max_length=400,
    return_tensors="pt"
)["input_ids"]

output_ids = model.generate(
    input_ids=input_ids,
    no_repeat_ngram_size=3,
    num_beams=5,
    early_stopping=True
)[0]

summary = tokenizer.decode(output_ids, skip_special_tokens=True)
print(summary)
```


